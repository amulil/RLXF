{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b6969033-9655-4af2-9120-3080f77a3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a4b0e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "end_of_conversation_token = \"<|endoftext|>\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, padding_side='right')\n",
    "tokenizer.add_special_tokens(\n",
    "            {'additional_special_tokens': [end_of_conversation_token]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0646e8",
   "metadata": {},
   "source": [
    "## RModel Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c447ef8b-8b3a-464e-b24c-a532f2628f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dataset = []\n",
    "rejected_dataset = []\n",
    "data = [{\n",
    "    \"prompt\": \"背诵静夜思第一句\",\n",
    "    \"chosen\": \"床前明月光，疑是地上霜。\",\n",
    "    \"rejected\": \"等等。\"\n",
    "}]\n",
    "chosen_sentence = data[0][\"prompt\"] + data[0][\"chosen\"] + end_of_conversation_token\n",
    "reject_sentence = data[0][\"prompt\"] + data[0][\"rejected\"] + end_of_conversation_token\n",
    "chosen_dataset.append(tokenizer(\n",
    "    chosen_sentence,\n",
    "    max_length=48,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "))\n",
    "rejected_dataset.append(tokenizer(\n",
    "    reject_sentence,\n",
    "    max_length=48,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "))\n",
    "one_batch = {\n",
    "    \"input_ids\": torch.cat([chosen_dataset[0][\"input_ids\"], rejected_dataset[0][\"input_ids\"]], dim=0),\n",
    "    \"attention_mask\": torch.cat([chosen_dataset[0][\"attention_mask\"], rejected_dataset[0][\"attention_mask\"]], dim=0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f362d12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 48])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01dfac",
   "metadata": {},
   "source": [
    "## Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc2a6090-f1a7-4f90-ba3c-651d19ad437d",
   "metadata": {},
   "outputs": [],
   "source": [
    " class RewardModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_model,\n",
    "                 tokenizer,\n",
    "                 num_padding_at_beginning=0,\n",
    "                 compute_fp32_loss=False):\n",
    "        super().__init__()\n",
    "        self.config = base_model.config\n",
    "        self.num_padding_at_beginning = num_padding_at_beginning\n",
    "        # reward model 为了计算 reward 额外加的一层\n",
    "        self.v_head = nn.Linear(self.config.hidden_size, 1, bias=False)\n",
    "        self.rwtransformer = base_model\n",
    "        self.PAD_ID = tokenizer.pad_token_id\n",
    "        self.compute_fp32_loss = compute_fp32_loss\n",
    "\n",
    "    def gradient_checkpointing_enable(self):\n",
    "        self.rwtransformer.gradient_checkpointing_enable()\n",
    "\n",
    "    def gradient_checkpointing_disable(self):\n",
    "        self.rwtransformer.gradient_checkpointing_disable()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                past_key_values=None,\n",
    "                attention_mask=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                use_cache=False):\n",
    "        \n",
    "        loss = None\n",
    "        # base model 的输出\n",
    "        transformer_outputs = self.rwtransformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache)\n",
    "        # 取最后一层的隐层输出，然后输入到 v_head\n",
    "        # one_batch shape: [2, 48, 2048] -> [2, 48, 1] -> [2,48]，所以相当于每个 token 有一个 reward\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        print(f\"hidden_states.shape: {hidden_states.shape}\")\n",
    "        rewards = self.v_head(hidden_states).squeeze(-1)\n",
    "        print(f\"rewards.shape: {rewards.shape}\")\n",
    "        chosen_mean_scores = []\n",
    "        rejected_mean_scores = []\n",
    "\n",
    "        # Split the inputs and rewards into two parts, chosen and rejected\n",
    "        assert len(input_ids.shape) == 2\n",
    "        bs = input_ids.shape[0] // 2 # 对于 one_batch，bs 是 1\n",
    "        seq_len = input_ids.shape[1]\n",
    "\n",
    "        chosen_ids = input_ids[:bs]  # bs x seq x 1\n",
    "        rejected_ids = input_ids[bs:]\n",
    "        chosen_rewards = rewards[:bs]\n",
    "        rejected_rewards = rewards[bs:]\n",
    "\n",
    "        # Compute pairwise loss. Only backprop on the different tokens before padding\n",
    "        loss = 0.\n",
    "        for i in range(bs):\n",
    "            chosen_id = chosen_ids[i]\n",
    "            rejected_id = rejected_ids[i]\n",
    "            chosen_reward = chosen_rewards[i]\n",
    "            rejected_reward = rejected_rewards[i]\n",
    "\n",
    "            c_inds = (chosen_id == self.PAD_ID).nonzero()\n",
    "            print(f\"c_inds: {c_inds}\")\n",
    "            # chosen data 的结束位置 c_ind，c_inds[self.num_padding_at_beginning].item() 是第一个 padding token 的位置\n",
    "            c_ind = c_inds[self.num_padding_at_beginning].item() if len(\n",
    "                c_inds\n",
    "            ) > self.num_padding_at_beginning else seq_len  # OPT model pads the first token, so we need to use the second padding token as the end of the sequence\n",
    "            # 看 chosen 和 rejected 是从哪个 token 开始不同的\n",
    "            check_divergence = (chosen_id != rejected_id).nonzero()\n",
    "\n",
    "            # 如果没有不同的 token，那么 c_ind 和 r_ind 一样\n",
    "            if len(check_divergence) == 0:\n",
    "                end_ind = rejected_reward.size(-1)\n",
    "                divergence_ind = end_ind - 1\n",
    "                r_ind = c_ind\n",
    "            else:\n",
    "                # Check if there is any padding otherwise take length of sequence\n",
    "                r_inds = (rejected_id == self.PAD_ID).nonzero()\n",
    "                r_ind = r_inds[self.num_padding_at_beginning].item(\n",
    "                ) if len(r_inds) > self.num_padding_at_beginning else seq_len\n",
    "                # 如果有不同的 token，那么 c_ind 和 r_ind 里更大的那个是结束 token 的位置\n",
    "                end_ind = max(c_ind, r_ind)\n",
    "                # divergence_ind 是第一个不同的 token 的位置，reward 也是从这个位置开始不同的\n",
    "                divergence_ind = check_divergence[0]\n",
    "            assert divergence_ind > 0\n",
    "            c_truncated_reward = chosen_reward[divergence_ind:end_ind]\n",
    "            print(\"c_truncated_reward last_token reward: \", c_truncated_reward[-1])\n",
    "            r_truncated_reward = rejected_reward[divergence_ind:end_ind]\n",
    "            chosen_mean_scores.append(\n",
    "                chosen_reward[c_ind - 1])  #use the end score for reference\n",
    "            rejected_mean_scores.append(rejected_reward[r_ind - 1])\n",
    "\n",
    "            if self.compute_fp32_loss:\n",
    "                c_truncated_reward = c_truncated_reward.float()\n",
    "                r_truncated_reward = r_truncated_reward.float()\n",
    "            loss += -torch.nn.functional.logsigmoid(c_truncated_reward -\n",
    "                                                    r_truncated_reward).mean()\n",
    "\n",
    "        # loss 这里是用的是所有 token 的 reward 的平均值，但是也有用最后一个 token 的 reward 的实现，这两种实现应该是都可以的\n",
    "        # loss += -torch.nn.functional.logsigmoid(c_truncated_reward[-1] -\n",
    "        #                                            r_truncated_reward[-1])\n",
    "        loss = loss / bs\n",
    "        chosen_mean_scores = torch.stack(chosen_mean_scores)\n",
    "        rejected_mean_scores = torch.stack(rejected_mean_scores)\n",
    "        # chosen_mean_scores 实际上是 end_of_conversation_token 的 reward，这里叫 mean 有些奇怪\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"chosen_mean_scores\": chosen_mean_scores,\n",
    "            \"rejected_mean_scores\": rejected_mean_scores,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "393f4ccb-c7a3-4882-8e65-81026a4d26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModel.from_pretrained(model_path)\n",
    "base_model.resize_token_embeddings(int(\n",
    "        8 *\n",
    "        math.ceil(len(tokenizer) / 8.0))) \n",
    "critic_model = RewardModel(base_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9ccf41e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states.shape: torch.Size([2, 48, 2048])\n",
      "rewards.shape: torch.Size([2, 48])\n",
      "c_inds: tensor([[37],\n",
      "        [38],\n",
      "        [39],\n",
      "        [40],\n",
      "        [41],\n",
      "        [42],\n",
      "        [43],\n",
      "        [44],\n",
      "        [45],\n",
      "        [46],\n",
      "        [47]])\n",
      "c_truncated_reward last_token reward:  tensor(0.0040, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(0.4015, grad_fn=<DivBackward0>),\n",
       " 'chosen_mean_scores': tensor([0.0040], grad_fn=<StackBackward0>),\n",
       " 'rejected_mean_scores': tensor([0.2623], grad_fn=<StackBackward0>)}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_model(**one_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864a7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
